{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4d475344",
      "metadata": {},
      "source": [
        "Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4dd980cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "95ed169f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset extracted to: papila_dataset\n"
          ]
        }
      ],
      "source": [
        "\n",
        "zip_path = \"PAPILA.zip\"   # dataset zip file\n",
        "extract_dir = \"papila_dataset\"    # folder to extract\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Dataset extracted to:\", extract_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f0d8de08",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "95f69bb0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OD shape: (246, 13)\n",
            "OS shape: (246, 13)\n"
          ]
        }
      ],
      "source": [
        "# Load right (OD) and left (OS) eye data\n",
        "od_df = pd.read_excel(\"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/ClinicalData/patient_data_od.xlsx\")\n",
        "os_df = pd.read_excel(\"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/ClinicalData/patient_data_os.xlsx\")\n",
        "\n",
        "print(\"OD shape:\", od_df.shape)\n",
        "print(\"OS shape:\", os_df.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3a732d89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Unnamed: 0  Age  Gender  Diagnosis Refractive_Defect Unnamed: 5  \\\n",
            "0        NaN  Age  Gender  Diagnosis         dioptre_1  dioptre_2   \n",
            "1         ID  NaN     NaN        NaN               NaN        NaN   \n",
            "2       #002   47       0          2              0.75      -1.75   \n",
            "3       #004   58       1          1               1.5      -1.75   \n",
            "4       #005   89       1          1             -0.75      -1.25   \n",
            "\n",
            "    Unnamed: 6  Phakic/Pseudophakic        IOP Unnamed: 9  Pachymetry  \\\n",
            "0  astigmatism  Phakic/Pseudophakic  Pneumatic    Perkins  Pachymetry   \n",
            "1          NaN                  NaN        NaN        NaN         NaN   \n",
            "2           90                    0         21        NaN         586   \n",
            "3           85                    0        NaN         19         501   \n",
            "4          101                    1         13         14         565   \n",
            "\n",
            "   Axial_Length  VF_MD  \n",
            "0  Axial_Length  VF_MD  \n",
            "1           NaN    NaN  \n",
            "2         23.64  -0.07  \n",
            "3         23.06  -3.26  \n",
            "4         23.81 -14.98  \n",
            "  Unnamed: 0  Age  Gender  Diagnosis Refractive_Defect Unnamed: 5  \\\n",
            "0        NaN  Age  Gender  Diagnosis         dioptre_1  dioptre_2   \n",
            "1         ID  NaN     NaN        NaN               NaN        NaN   \n",
            "2       #002   47       0          2              -0.5       -1.5   \n",
            "3       #004   58       1          1               1.5       -2.5   \n",
            "4       #005   89       1          1              -0.5         -2   \n",
            "\n",
            "    Unnamed: 6  Phakic/Pseudophakic        IOP Unnamed: 9  Pachymetry  \\\n",
            "0  astigmatism  Phakic/Pseudophakic  Pneumatic    Perkins  Pachymetry   \n",
            "1          NaN                  NaN        NaN        NaN         NaN   \n",
            "2           88                    0         20        NaN         603   \n",
            "3           85                    1        NaN         19         511   \n",
            "4          100                    1         24         10         575   \n",
            "\n",
            "   Axial_Length  VF_MD  \n",
            "0  Axial_Length  VF_MD  \n",
            "1           NaN    NaN  \n",
            "2         23.77   0.17  \n",
            "3         22.96  -6.77  \n",
            "4         24.33  -7.44  \n"
          ]
        }
      ],
      "source": [
        "print(od_df.head())\n",
        "print(os_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "19f5757c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OD columns: Index(['Unnamed: 0', 'Age', 'Gender', 'Diagnosis', 'Refractive_Defect',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Phakic/Pseudophakic', 'IOP', 'Unnamed: 9',\n",
            "       'Pachymetry', 'Axial_Length', 'VF_MD'],\n",
            "      dtype='object')\n",
            "OS columns: Index(['Unnamed: 0', 'Age', 'Gender', 'Diagnosis', 'Refractive_Defect',\n",
            "       'Unnamed: 5', 'Unnamed: 6', 'Phakic/Pseudophakic', 'IOP', 'Unnamed: 9',\n",
            "       'Pachymetry', 'Axial_Length', 'VF_MD'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(\"OD columns:\", od_df.columns)\n",
        "print(\"OS columns:\", os_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "cc683fd8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OD cleaned shape: (244, 13)\n",
            "OS cleaned shape: (244, 13)\n",
            "  Unnamed: 0  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07  \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26  \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98  \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07  \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30  \n"
          ]
        }
      ],
      "source": [
        "    \n",
        "# Load OD (right eye), skip first 2 rows\n",
        "od_df = pd.read_excel(\n",
        "    \"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/ClinicalData/patient_data_od.xlsx\", \n",
        "    skiprows=[0,2], engine=\"openpyxl\"\n",
        ")\n",
        "\n",
        "# Load OS (left eye), skip first 2 rows\n",
        "os_df = pd.read_excel(\n",
        "    \"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/ClinicalData/patient_data_os.xlsx\", \n",
        "    skiprows=[0,2], engine=\"openpyxl\"\n",
        ")\n",
        "\n",
        "print(\"OD cleaned shape:\", od_df.shape)\n",
        "print(\"OS cleaned shape:\", os_df.shape)\n",
        "print(od_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "5fa0823a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Right eye data (OD):\n",
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07  \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26  \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98  \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07  \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30  \n",
            "Left eye data (OS):\n",
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2      -0.50       -1.5         88.0   \n",
            "1       #004   58       1          1       1.50       -2.5         85.0   \n",
            "2       #005   89       1          1      -0.50       -2.0        100.0   \n",
            "3       #006   69       0          2       1.00       -1.5         85.0   \n",
            "4       #007   22       1          2      -0.25       -0.5          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \n",
            "0                  0.0       20.0      NaN       603.0         23.77   0.17  \n",
            "1                  1.0        NaN     19.0       511.0         22.96  -6.77  \n",
            "2                  1.0       24.0     10.0       575.0         24.33  -7.44  \n",
            "3                  0.0       22.0      NaN       593.0         26.21  -3.31  \n",
            "4                  0.0       13.0      NaN         NaN         23.35  -2.61  \n"
          ]
        }
      ],
      "source": [
        "# Rename Unnamed: 0 to Patient_ID\n",
        "od_df.rename(columns={\"Unnamed: 0\": \"Patient_ID\"}, inplace=True)\n",
        "os_df.rename(columns={\"Unnamed: 0\": \"Patient_ID\"}, inplace=True)\n",
        "\n",
        "print(\"Right eye data (OD):\")\n",
        "print(od_df.head())\n",
        "print (\"Left eye data (OS):\")\n",
        "print(os_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6ae9973e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined clinical data shape: (488, 14)\n",
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \\\n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07   \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26   \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98   \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07   \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30   \n",
            "\n",
            "  Eye_Label  \n",
            "0        OD  \n",
            "1        OD  \n",
            "2        OD  \n",
            "3        OD  \n",
            "4        OD  \n"
          ]
        }
      ],
      "source": [
        "od_df[\"Eye_Label\"] = \"OD\"\n",
        "os_df[\"Eye_Label\"] = \"OS\"\n",
        "\n",
        "clinical_df = pd.concat([od_df, os_df], axis=0).reset_index(drop=True)\n",
        "print(\"Combined clinical data shape:\", clinical_df.shape)\n",
        "print(clinical_df.head())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9c40c45e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final shape: (488, 14)\n",
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \\\n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07   \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26   \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98   \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07   \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30   \n",
            "\n",
            "  Eye_Label  \n",
            "0        OD  \n",
            "1        OD  \n",
            "2        OD  \n",
            "3        OD  \n",
            "4        OD  \n"
          ]
        }
      ],
      "source": [
        "# Drop the duplicate eye column\n",
        "clinical_df = clinical_df.drop(columns=[\"Eye Label\"], errors=\"ignore\")\n",
        "\n",
        "print(\"Final shape:\", clinical_df.shape)\n",
        "print(clinical_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "9197bc9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final shape: (488, 14)\n",
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \\\n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07   \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26   \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98   \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07   \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30   \n",
            "\n",
            "  Eye_Label  \n",
            "0        OD  \n",
            "1        OD  \n",
            "2        OD  \n",
            "3        OD  \n",
            "4        OD  \n"
          ]
        }
      ],
      "source": [
        "# Drop the duplicate eye column\n",
        "clinical_df = clinical_df.drop(columns=[\"Eye\"], errors=\"ignore\")\n",
        "\n",
        "print(\"Final shape:\", clinical_df.shape)\n",
        "print(clinical_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "896ef935",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0       #002   47       0          2       0.75      -1.75         90.0   \n",
            "1       #004   58       1          1       1.50      -1.75         85.0   \n",
            "2       #005   89       1          1      -0.75      -1.25        101.0   \n",
            "3       #006   69       0          2       1.00      -1.50         95.0   \n",
            "4       #007   22       1          2      -0.25       0.00          0.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \\\n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07   \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26   \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98   \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07   \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30   \n",
            "\n",
            "  Eye_Label  \n",
            "0        OD  \n",
            "1        OD  \n",
            "2        OD  \n",
            "3        OD  \n",
            "4        OD  \n"
          ]
        }
      ],
      "source": [
        "print(clinical_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "277e55a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Patient_ID', 'Age', 'Gender', 'Diagnosis', 'dioptre_1', 'dioptre_2',\n",
            "       'astigmatism', 'Phakic/Pseudophakic', 'Pneumatic', 'Perkins',\n",
            "       'Pachymetry', 'Axial_Length', 'VF_MD', 'Eye_Label'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print (clinical_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "868dc437",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final multimodal dataset shape: (0, 15)\n"
          ]
        }
      ],
      "source": [
        "import glob, os\n",
        "\n",
        "# Get all images\n",
        "image_paths = glob.glob(\"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/FundusImages/*.jpg\")\n",
        "\n",
        "# Make dataframe of images\n",
        "image_df = pd.DataFrame({\n",
        "    \"Patient_ID\": [os.path.basename(p)[3:6] for p in image_paths],  \n",
        "    \"Eye_Label\": [os.path.basename(p)[6:8] for p in image_paths],          \n",
        "    \"Image_Path\": image_paths\n",
        "})\n",
        "\n",
        "# Merge with clinical data\n",
        "multimodal_df = pd.merge(clinical_df, image_df, on=[\"Patient_ID\",\"Eye_Label\"], how=\"inner\")\n",
        "\n",
        "print(\"Final multimodal dataset shape:\", multimodal_df.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9db1b4fa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Patient_ID Eye_Label                                         Image_Path\n",
            "0        210        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...\n",
            "1        002        OS  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...\n",
            "2        266        OS  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...\n",
            "3        051        OS  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...\n",
            "4        179        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...\n"
          ]
        }
      ],
      "source": [
        "print(image_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "498ce381",
      "metadata": {},
      "outputs": [],
      "source": [
        "clinical_df[\"Patient_ID\"] = clinical_df[\"Patient_ID\"].str.replace(\"#\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "959b48c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0        002   47       0          2       0.75      -1.75         90.0   \n",
            "1        004   58       1          1       1.50      -1.75         85.0   \n",
            "2        005   89       1          1      -0.75      -1.25        101.0   \n",
            "3        006   69       0          2       1.00      -1.50         95.0   \n",
            "4        007   22       1          2      -0.25       0.00          0.0   \n",
            "5        008   67       1          2        NaN      -0.75         20.0   \n",
            "6        009   79       0          2       0.75      -1.50         95.0   \n",
            "7        010   72       1          1       2.25      -1.50        105.0   \n",
            "8        013   70       1          1       3.00      -1.00         65.0   \n",
            "9        014   60       1          1       0.25      -0.50        155.0   \n",
            "\n",
            "   Phakic/Pseudophakic  Pneumatic  Perkins  Pachymetry  Axial_Length  VF_MD  \\\n",
            "0                  0.0       21.0      NaN       586.0         23.64  -0.07   \n",
            "1                  0.0        NaN     19.0       501.0         23.06  -3.26   \n",
            "2                  1.0       13.0     14.0       565.0         23.81 -14.98   \n",
            "3                  0.0       22.0      NaN       612.0         26.25  -2.07   \n",
            "4                  0.0       14.0      NaN         NaN         23.39  -2.30   \n",
            "5                  0.0       15.0      NaN       526.0         22.92  -0.28   \n",
            "6                  0.0       23.0     20.0       511.0           NaN  -3.69   \n",
            "7                  0.0       16.0      NaN       590.0         22.46 -12.58   \n",
            "8                  0.0       13.0     17.0       500.0         22.75 -14.24   \n",
            "9                  0.0       11.0     14.0       505.0         23.81 -11.37   \n",
            "\n",
            "  Eye_Label                                         Image_Path  \n",
            "0        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "1        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "2        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "3        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "4        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "5        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "6        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "7        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "8        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "9        OD  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n"
          ]
        }
      ],
      "source": [
        "multimodal_df = pd.merge(clinical_df, image_df, on=[\"Patient_ID\",\"Eye_Label\"], how=\"inner\")\n",
        "print(multimodal_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "49fc4c10",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perkins                73.770492\n",
            "VF_MD                  66.393443\n",
            "Pneumatic              18.852459\n",
            "dioptre_1               5.327869\n",
            "Pachymetry              2.868852\n",
            "Phakic/Pseudophakic     2.049180\n",
            "Axial_Length            1.844262\n",
            "astigmatism             1.639344\n",
            "dioptre_2               1.639344\n",
            "Age                     0.000000\n",
            "Gender                  0.000000\n",
            "Diagnosis               0.000000\n",
            "Patient_ID              0.000000\n",
            "Eye_Label               0.000000\n",
            "Image_Path              0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "missing_percent = multimodal_df.isna().mean() * 100\n",
        "print(missing_percent.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "7c05a1d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned dataset shape: (488, 13)\n",
            "    Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
            "0          002   47       0          2       0.75      -1.75         90.0   \n",
            "1          004   58       1          1       1.50      -1.75         85.0   \n",
            "2          005   89       1          1      -0.75      -1.25        101.0   \n",
            "3          006   69       0          2       1.00      -1.50         95.0   \n",
            "4          007   22       1          2      -0.25       0.00          0.0   \n",
            "..         ...  ...     ...        ...        ...        ...          ...   \n",
            "483        289   64       0          0       0.75      -1.50         93.0   \n",
            "484        290   75       1          0       0.25      -0.25        160.0   \n",
            "485        291   55       0          0       1.50      -1.25         76.0   \n",
            "486        292   56       1          0       1.25      -0.75         79.0   \n",
            "487        293   39       1          0      -0.75      -0.25        110.0   \n",
            "\n",
            "     Phakic/Pseudophakic  Pneumatic  Pachymetry  Axial_Length Eye_Label  \\\n",
            "0                    0.0       21.0       586.0         23.64        OD   \n",
            "1                    0.0       16.0       501.0         23.06        OD   \n",
            "2                    1.0       13.0       565.0         23.81        OD   \n",
            "3                    0.0       22.0       612.0         26.25        OD   \n",
            "4                    0.0       14.0       535.0         23.39        OD   \n",
            "..                   ...        ...         ...           ...       ...   \n",
            "483                  1.0       10.0       531.0         22.31        OS   \n",
            "484                  1.0       19.0       573.0         22.01        OS   \n",
            "485                  0.0       14.0       443.0         23.51        OS   \n",
            "486                  0.0        9.0       479.0         23.84        OS   \n",
            "487                  0.0       14.0       468.0         24.00        OS   \n",
            "\n",
            "                                            Image_Path  \n",
            "0    papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "1    papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "2    papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "3    papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "4    papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "..                                                 ...  \n",
            "483  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "484  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "485  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "486  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "487  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
            "\n",
            "[488 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "# Drop high-missing features\n",
        "multimodal_df= multimodal_df.drop(columns=[\"Perkins\", \"VF_MD\"])\n",
        "\n",
        "# Fill missing values in the rest\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")  \n",
        "cols_to_impute = [\"Pneumatic\", \"dioptre_1\", \"dioptre_2\", \n",
        "                  \"astigmatism\", \"Pachymetry\", \"Phakic/Pseudophakic\", \"Axial_Length\"]\n",
        "\n",
        "multimodal_df[cols_to_impute] = imputer.fit_transform(multimodal_df[cols_to_impute])\n",
        "\n",
        "print(\"Cleaned dataset shape:\", multimodal_df.shape)\n",
        "print(multimodal_df.head(600))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d1dc726e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from scikit-learn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4b209ae0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patient_ID             0.0\n",
            "Age                    0.0\n",
            "Gender                 0.0\n",
            "Diagnosis              0.0\n",
            "dioptre_1              0.0\n",
            "dioptre_2              0.0\n",
            "astigmatism            0.0\n",
            "Phakic/Pseudophakic    0.0\n",
            "Pneumatic              0.0\n",
            "Pachymetry             0.0\n",
            "Axial_Length           0.0\n",
            "Eye_Label              0.0\n",
            "Image_Path             0.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "missing_percent = multimodal_df.isna().mean() * 100\n",
        "print(missing_percent.sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5daf29de",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>Age</th>\n",
              "      <th>Gender</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>dioptre_1</th>\n",
              "      <th>dioptre_2</th>\n",
              "      <th>astigmatism</th>\n",
              "      <th>Phakic/Pseudophakic</th>\n",
              "      <th>Pneumatic</th>\n",
              "      <th>Pachymetry</th>\n",
              "      <th>Axial_Length</th>\n",
              "      <th>Eye_Label</th>\n",
              "      <th>Image_Path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>002</td>\n",
              "      <td>47</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>-1.75</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>586.0</td>\n",
              "      <td>23.64</td>\n",
              "      <td>OD</td>\n",
              "      <td>papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.50</td>\n",
              "      <td>-1.75</td>\n",
              "      <td>85.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>501.0</td>\n",
              "      <td>23.06</td>\n",
              "      <td>OD</td>\n",
              "      <td>papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>005</td>\n",
              "      <td>89</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>-1.25</td>\n",
              "      <td>101.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>23.81</td>\n",
              "      <td>OD</td>\n",
              "      <td>papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>006</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>-1.50</td>\n",
              "      <td>95.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>612.0</td>\n",
              "      <td>26.25</td>\n",
              "      <td>OD</td>\n",
              "      <td>papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>007</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>535.0</td>\n",
              "      <td>23.39</td>\n",
              "      <td>OD</td>\n",
              "      <td>papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Patient_ID  Age  Gender  Diagnosis  dioptre_1  dioptre_2  astigmatism  \\\n",
              "0        002   47       0          2       0.75      -1.75         90.0   \n",
              "1        004   58       1          1       1.50      -1.75         85.0   \n",
              "2        005   89       1          1      -0.75      -1.25        101.0   \n",
              "3        006   69       0          2       1.00      -1.50         95.0   \n",
              "4        007   22       1          2      -0.25       0.00          0.0   \n",
              "\n",
              "   Phakic/Pseudophakic  Pneumatic  Pachymetry  Axial_Length Eye_Label  \\\n",
              "0                  0.0       21.0       586.0         23.64        OD   \n",
              "1                  0.0       16.0       501.0         23.06        OD   \n",
              "2                  1.0       13.0       565.0         23.81        OD   \n",
              "3                  0.0       22.0       612.0         26.25        OD   \n",
              "4                  0.0       14.0       535.0         23.39        OD   \n",
              "\n",
              "                                          Image_Path  \n",
              "0  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
              "1  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
              "2  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
              "3  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  \n",
              "4  papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multimodal_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ddd52004",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(488, 13)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multimodal_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f54681be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map 1 and 2 to 1 (glaucoma), 0 stays 0 (healthy)\n",
        "multimodal_df[\"Diagnosis\"] = multimodal_df[\"Diagnosis\"].map({\n",
        "    0: 0,   # healthy\n",
        "    1: 1,   # suspect → glaucoma\n",
        "    2: 1    # glaucoma\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "c55e9e2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# multimodal_df[\"Diagnosis\"] = multimodal_df[\"Diagnosis\"].map({1: 0, 2: 1})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd15bbb8",
      "metadata": {},
      "source": [
        "Dataset Preparation for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "2ee7abd7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 341  Val: 98  Test: 49\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First: 70% train, 30% temp\n",
        "train_df, temp_df = train_test_split(\n",
        "    multimodal_df,\n",
        "    test_size=0.3,\n",
        "    stratify=multimodal_df[\"Diagnosis\"],  \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Then: split temp (30%) into 20% val and 10% test\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    test_size=0.33,  # 0.33 * 0.3 ≈ 0.10 of total\n",
        "    stratify=temp_df[\"Diagnosis\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_df), \" Val:\", len(val_df), \" Test:\", len(test_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ab09f1",
      "metadata": {},
      "source": [
        "Diffusion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "dd730faa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/FundusImages/RET033OS.jpg'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.iloc[-1, -1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b200353a",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "37db4d55",
      "metadata": {},
      "source": [
        "Create a PyTorch Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "92430c8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import Dataset\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# class PapilaDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None):\n",
        "#         self.df = dataframe.reset_index(drop=True)\n",
        "#         self.transform = transform\n",
        "#         self.clinical_cols = [\"Age\",\"Gender\",\"Pneumatic\",\n",
        "#                               \"dioptre_1\",\"dioptre_2\",\"astigmatism\",\n",
        "#                               \"Pachymetry\",\"Phakic/Pseudophakic\",\"Axial_Length\"]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "\n",
        "#         # --- Load image ---\n",
        "#         img = Image.open(row[\"Image_Path\"]).convert(\"RGB\")\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         # --- Clinical features as tensor (force numeric) ---\n",
        "#         clinical_values = row[self.clinical_cols].astype(float).values\n",
        "#         clinical = torch.tensor(clinical_values, dtype=torch.float32)\n",
        "\n",
        "#         # --- Label ---\n",
        "#         label = torch.tensor(int(row[\"Diagnosis\"]), dtype=torch.long)\n",
        "\n",
        "#         return img, clinical, label\n",
        "    \n",
        "# import torchvision.transforms as T\n",
        "\n",
        "# transform = T.Compose([\n",
        "#     T.Resize((224,224)),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "# ])\n",
        "\n",
        "# # Pass transform when creating datasets\n",
        "# train_dataset = PapilaDataset(train_df, transform=transform)\n",
        "# val_dataset   = PapilaDataset(val_df, transform=transform)\n",
        "# test_dataset  = PapilaDataset(test_df, transform=transform)\n",
        "\n",
        "# img, clinical, label = train_dataset[0]\n",
        "# print(\"Image tensor shape:\", img.shape)  # ➜ torch.Size([3, 224, 224])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "6cbb5404",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Instantiate datasets (no transform yet)\n",
        "# train_dataset = PapilaDataset(train_df)\n",
        "# val_dataset   = PapilaDataset(val_df)\n",
        "# test_dataset  = PapilaDataset(test_df)\n",
        "\n",
        "# # Get the first sample from train_dataset\n",
        "# img, clinical, label = train_dataset[150]\n",
        "\n",
        "# # Print the types and values\n",
        "# print(\"Image type:\", type(img))           # ➜ <class 'PIL.Image.Image'>\n",
        "# print(\"Clinical features:\", clinical)     # ➜ torch.Tensor([...])\n",
        "# print(\"Label:\", label)                    # ➜ torch.Tensor(0/1/2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "bd2139fb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Distribution:\n",
            "Diagnosis\n",
            "0    233\n",
            "1    108\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "class_counts = train_df[\"Diagnosis\"].value_counts()\n",
        "print(\"Class Distribution:\")\n",
        "print(class_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92be5313",
      "metadata": {},
      "source": [
        "Dataset class for training DDPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "48b0c982",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "019c7cca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import torchvision.transforms as T\n",
        "# from diffusers import UNet2DModel, DDPMScheduler\n",
        "# import torch.nn.functional as F\n",
        "# from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# # --- Dataset ---\n",
        "# class PapilaDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None):\n",
        "#         self.df = dataframe.reset_index(drop=True)\n",
        "#         self.transform = transform\n",
        "#         self.clinical_cols = [\"Age\",\"Gender\",\"Pneumatic\",\n",
        "#                               \"dioptre_1\",\"dioptre_2\",\"astigmatism\",\n",
        "#                               \"Pachymetry\",\"Phakic/Pseudophakic\",\"Axial_Length\"]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         img = Image.open(row[\"Image_Path\"]).convert(\"RGB\")\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         clinical_values = row[self.clinical_cols].astype(float).values\n",
        "#         clinical = torch.tensor(clinical_values, dtype=torch.float32)\n",
        "#         label = torch.tensor(int(row[\"Diagnosis\"]), dtype=torch.long)\n",
        "\n",
        "#         return img, clinical, label\n",
        "\n",
        "# # --- Transform ---\n",
        "# transform = T.Compose([\n",
        "#     T.Resize((128, 128)),\n",
        "#     T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize([0.5]*3, [0.5]*3)\n",
        "# ])\n",
        "\n",
        "\n",
        "# # --- Datasets & Dataloaders ---\n",
        "# train_dataset = PapilaDataset(train_df, transform=transform)\n",
        "# val_dataset   = PapilaDataset(val_df, transform=transform)\n",
        "# test_dataset  = PapilaDataset(test_df, transform=transform)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
        "# val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "# test_loader  = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# # --- DDPM Model ---\n",
        "# model = UNet2DModel(\n",
        "#     sample_size=128,\n",
        "#     in_channels=3,\n",
        "#     out_channels=3,\n",
        "#     layers_per_block=2,\n",
        "#    block_out_channels=(64, 128, 128),\n",
        " \n",
        "#     down_block_types=(\"DownBlock2D\",) * 3,\n",
        "#     up_block_types=(\"UpBlock2D\",) * 3,\n",
        "# )\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # --- Scheduler and Optimizer ---\n",
        "# noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# # --- Training Loop ---\n",
        "# model.train()\n",
        "# for epoch in range(200):\n",
        "#     for batch in train_loader:\n",
        "#         clean_images, _, _ = batch\n",
        "#         clean_images = clean_images.to(device)\n",
        "\n",
        "#         noise = torch.randn_like(clean_images)\n",
        "#         timesteps = torch.randint(\n",
        "#             0, noise_scheduler.config.num_train_timesteps, (clean_images.size(0),), device=device\n",
        "#         ).long()\n",
        "\n",
        "#         noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "#         # Forward pass\n",
        "#         noise_pred = model(noisy_images, timesteps).sample\n",
        "#         loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "776d0624",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diffusers[torch]\n",
            "  Downloading diffusers-0.35.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting importlib_metadata (from diffusers[torch])\n",
            "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: filelock in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from diffusers[torch]) (3.19.1)\n",
            "Collecting huggingface-hub>=0.34.0 (from diffusers[torch])\n",
            "  Downloading huggingface_hub-1.0.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from diffusers[torch]) (2.2.2)\n",
            "Collecting regex!=2019.12.17 (from diffusers[torch])\n",
            "  Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: requests in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from diffusers[torch]) (2.32.5)\n",
            "Collecting safetensors>=0.3.1 (from diffusers[torch])\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from diffusers[torch]) (11.3.0)\n",
            "Requirement already satisfied: torch>=1.4 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from diffusers[torch]) (2.6.0+cu124)\n",
            "Collecting accelerate>=0.31.0 (from diffusers[torch])\n",
            "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from accelerate>=0.31.0->diffusers[torch]) (25.0)\n",
            "Requirement already satisfied: psutil in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from accelerate>=0.31.0->diffusers[torch]) (7.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from accelerate>=0.31.0->diffusers[torch]) (6.0.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from huggingface-hub>=0.34.0->diffusers[torch]) (2025.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from huggingface-hub>=0.34.0->diffusers[torch]) (0.28.1)\n",
            "Collecting shellingham (from huggingface-hub>=0.34.0->diffusers[torch])\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from huggingface-hub>=0.34.0->diffusers[torch]) (4.67.1)\n",
            "Collecting typer-slim (from huggingface-hub>=0.34.0->diffusers[torch])\n",
            "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from huggingface-hub>=0.34.0->diffusers[torch]) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.34.0->diffusers[torch])\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: anyio in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (4.11.0)\n",
            "Requirement already satisfied: certifi in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (1.0.9)\n",
            "Requirement already satisfied: idna in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (0.16.0)\n",
            "Requirement already satisfied: networkx in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (70.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from torch>=1.4->diffusers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.4->diffusers[torch]) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.34.0->diffusers[torch]) (1.3.1)\n",
            "Collecting zipp>=3.20 (from importlib_metadata->diffusers[torch])\n",
            "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from jinja2->torch>=1.4->diffusers[torch]) (2.1.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from requests->diffusers[torch]) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/s25afsha/venvs/linuxlab/lib/python3.12/site-packages (from requests->diffusers[torch]) (2.5.0)\n",
            "Collecting click>=8.0.0 (from typer-slim->huggingface-hub>=0.34.0->diffusers[torch])\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading diffusers-0.35.2-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
            "Downloading huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
            "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
            "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Installing collected packages: zipp, shellingham, safetensors, regex, hf-xet, click, typer-slim, importlib_metadata, huggingface-hub, diffusers, accelerate\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [accelerate]1\u001b[0m [accelerate]-hub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 click-8.3.0 diffusers-0.35.2 hf-xet-1.2.0 huggingface-hub-1.0.1 importlib_metadata-8.7.0 regex-2025.10.23 safetensors-0.6.2 shellingham-1.5.4 typer-slim-0.20.0 zipp-3.23.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install diffusers[torch]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "67396453",
      "metadata": {},
      "outputs": [],
      "source": [
        "# from diffusers import DDPMPipeline\n",
        "# import os\n",
        "\n",
        "# # Create output folder\n",
        "# os.makedirs(\"synthetic_fundus\", exist_ok=True)\n",
        "\n",
        "# # Switch to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# # Create pipeline from trained model\n",
        "# pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler).to(device)\n",
        "\n",
        "# # Number of images to generate\n",
        "# num_images = 10\n",
        "\n",
        "# # Sampling loop\n",
        "# for i in range(num_images):\n",
        "#     with torch.no_grad():\n",
        "#         output = pipeline(num_inference_steps=1000) \n",
        "#         image = output.images[0]\n",
        "#         image.save(f\"synthetic_fundus/gen_{i}.png\")\n",
        "\n",
        "# print(f\"{num_images} synthetic fundus images saved in /synthetic_fundus/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "e9be6b19",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "4644c197",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "2a1db0c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Imports ---\n",
        "# import os\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from PIL import Image\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import torch.nn.functional as F\n",
        "# import torchvision.transforms as T\n",
        "# from torchvision.utils import save_image\n",
        "# from diffusers import UNet2DModel, DDPMScheduler\n",
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "# from skimage.exposure import match_histograms\n",
        "\n",
        "# # ========== 0) Reproducibility ==========\n",
        "# def set_seed(seed=42):\n",
        "#     torch.manual_seed(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.manual_seed_all(seed)\n",
        "# set_seed(42)\n",
        "\n",
        "# # ========== 1) Color normalize (histogram matching) ==========\n",
        "# def color_normalize(pil_img, ref_img):\n",
        "#     img = np.array(pil_img)\n",
        "#     ref = np.array(ref_img)\n",
        "#     matched = match_histograms(img, ref, channel_axis=-1)\n",
        "#     return Image.fromarray(matched.astype(np.uint8))\n",
        "\n",
        "# # ========== 2) CLAHE (mild) ==========\n",
        "# def apply_clahe(pil_img, clip=1.5, grid=8):\n",
        "#     img_cv = np.array(pil_img)\n",
        "#     lab = cv2.cvtColor(img_cv, cv2.COLOR_RGB2LAB)\n",
        "#     l, a, b = cv2.split(lab)\n",
        "#     clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(grid, grid))\n",
        "#     cl = clahe.apply(l)\n",
        "#     merged = cv2.merge((cl, a, b))\n",
        "#     enhanced_img = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "#     return Image.fromarray(enhanced_img)\n",
        "\n",
        "# # ========== 3) Dataset ==========\n",
        "# class PapilaDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None,\n",
        "#                  use_clahe=True, use_hist_match=True, ref_img=None):\n",
        "#         self.df = dataframe.reset_index(drop=True)\n",
        "#         self.transform = transform\n",
        "#         self.use_clahe = use_clahe\n",
        "#         self.use_hist_match = use_hist_match\n",
        "#         self.ref_img = ref_img\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.df)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         row = self.df.iloc[idx]\n",
        "#         img = Image.open(row[\"Image_Path\"]).convert(\"RGB\")\n",
        "\n",
        "#         if self.use_clahe:\n",
        "#             img = apply_clahe(img, clip=1.5, grid=8)\n",
        "\n",
        "#         if self.use_hist_match and self.ref_img is not None:\n",
        "#             img = color_normalize(img, self.ref_img)\n",
        "\n",
        "#         if self.transform:\n",
        "#             img = self.transform(img)\n",
        "#         return img\n",
        "\n",
        "# # ========== 4) Transforms ==========\n",
        "# transform = T.Compose([\n",
        "#     T.Resize(160),\n",
        "#     T.CenterCrop(128),\n",
        "#     T.ToTensor(),\n",
        "#     T.Normalize([0.5]*3, [0.5]*3)\n",
        "# ])\n",
        "\n",
        "# # ========== 5) Data ==========\n",
        "# REFERENCE_PATH = \"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/FundusImages/RET033OS.jpg\"\n",
        "# assert os.path.exists(REFERENCE_PATH), \"Set REFERENCE_PATH to a valid fundus image!\"\n",
        "# ref_img = Image.open(REFERENCE_PATH).convert(\"RGB\")\n",
        "\n",
        "# train_dataset = PapilaDataset(\n",
        "#     train_df,\n",
        "#     transform=transform,\n",
        "#     use_clahe=True,\n",
        "#     use_hist_match=True,\n",
        "#     ref_img=ref_img\n",
        "# )\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "# # ========== 6) Model ==========\n",
        "# model = UNet2DModel(\n",
        "#     sample_size=128, in_channels=3, out_channels=3,\n",
        "#     layers_per_block=2,\n",
        "#     block_out_channels=(64, 128, 128),\n",
        "#     down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
        "#     up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
        "# )\n",
        "# model.gradient_checkpointing = True\n",
        "\n",
        "# # ========== 7) EMA ==========\n",
        "# class EMA:\n",
        "#     def __init__(self, model, decay=0.999, device=\"cpu\"):\n",
        "#         self.decay = decay\n",
        "#         self.device = device\n",
        "#         self.shadow = {k: v.detach().clone().to(device)\n",
        "#                        for k, v in model.state_dict().items()}\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def update(self, model):\n",
        "#         for k, v in model.state_dict().items():\n",
        "#             self.shadow[k] = self.shadow[k].to(v.device)\n",
        "#             self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\n",
        "\n",
        "#     @torch.no_grad()\n",
        "#     def copy_to(self, model):\n",
        "#         model.load_state_dict(self.shadow, strict=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ========== 8) Setup ==========\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# noise_scheduler = DDPMScheduler(\n",
        "#     num_train_timesteps=1000,\n",
        "#     beta_schedule=\"squaredcos_cap_v2\",\n",
        "#     prediction_type=\"epsilon\"\n",
        "# )\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "# scaler = GradScaler()\n",
        "\n",
        "# os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "# os.makedirs(\"samples\", exist_ok=True)\n",
        "\n",
        "# def unnormalize(x):\n",
        "#     return (x * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "# # ========== 9) Sampling ==========\n",
        "# @torch.no_grad()\n",
        "# def sample_grid(cur_model, epoch, num=9, use_ema=True):\n",
        "#     cur_model.eval()\n",
        "#     tmp = UNet2DModel(**cur_model.config)\n",
        "#     tmp.load_state_dict(cur_model.state_dict(), strict=True)\n",
        "#     tmp.to(device)\n",
        "\n",
        "#     if use_ema:\n",
        "#         ema.copy_to(tmp)\n",
        "\n",
        "#     img = torch.randn((num, 3, 128, 128), device=device)\n",
        "#     for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
        "#         t_batch = torch.full((num,), t, device=device, dtype=torch.long)\n",
        "#         with autocast():\n",
        "#             eps = tmp(img, t_batch).sample\n",
        "#         img = noise_scheduler.step(eps, t, img).prev_sample\n",
        "\n",
        "#     img = unnormalize(img)\n",
        "#     save_image(img, f\"samples/gen_epoch{epoch}.png\", nrow=3)\n",
        "#     print(f\"Saved: samples/gen_epoch{epoch}.png\")\n",
        "\n",
        "# # ========== 10) Training ==========\n",
        "# def train_diffusion(cur_model, loader, epochs=180, sample_every=10):\n",
        "#     cur_model.train()\n",
        "#     global_step = 0\n",
        "\n",
        "#     for epoch in range(1, epochs+1):\n",
        "#         for batch in loader:\n",
        "#             clean = batch.to(device, non_blocking=True)\n",
        "\n",
        "#             noise = torch.randn_like(clean)\n",
        "#             t = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
        "#                               (clean.size(0),), device=device).long()\n",
        "#             noisy = noise_scheduler.add_noise(clean, noise, t)\n",
        "\n",
        "#             with autocast():\n",
        "#                 pred = cur_model(noisy, t).sample\n",
        "#                 loss = F.mse_loss(pred, noise)\n",
        "\n",
        "#             optimizer.zero_grad(set_to_none=True)\n",
        "#             scaler.scale(loss).backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(cur_model.parameters(), 1.0)\n",
        "#             scaler.step(optimizer)\n",
        "#             scaler.update()\n",
        "\n",
        "#             ema.update(cur_model)\n",
        "#             global_step += 1\n",
        "\n",
        "#         print(f\"Epoch {epoch}/{epochs}  |  loss: {loss.item():.4f}\")\n",
        "\n",
        "#         # save both normal and EMA weights\n",
        "#         torch.save(cur_model.state_dict(), f\"checkpoints/ddpm_epoch{epoch}.pt\")\n",
        "#         torch.save(ema.shadow, f\"checkpoints/ddpm_epoch{epoch}_ema.pt\")\n",
        "\n",
        "#         if epoch % sample_every == 0:\n",
        "#             sample_grid(cur_model, epoch, num=9, use_ema=True)\n",
        "\n",
        "# # ========== 11) Run ==========\n",
        "# if __name__ == \"__main__\":\n",
        "#     train_diffusion(model, train_loader, epochs=180, sample_every=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "42e1fb70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # --- Sampling (Reverse Diffusion) ---\n",
        "# os.makedirs(\"synthetic_fundus\", exist_ok=True)\n",
        "# model.eval()\n",
        "\n",
        "# pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler).to(device)\n",
        "\n",
        "# num_images = 10\n",
        "# for i in range(num_images):\n",
        "#     with torch.no_grad():\n",
        "#         output = pipeline(num_inference_steps=250)\n",
        "#         image = output.images[0]\n",
        "#         image.save(f\"synthetic_fundus/gen_{i}.png\")\n",
        "\n",
        "# print(f\"{num_images} synthetic fundus images saved in /synthetic_fundus/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564b88eb",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a8c7e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9311/2014063652.py:87: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipykernel_9311/2014063652.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# ============================\n",
        "# 1. CLAHE Enhancement\n",
        "# ============================\n",
        "def apply_clahe(pil_img):\n",
        "    img_cv = np.array(pil_img)\n",
        "    lab = cv2.cvtColor(img_cv, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    enhanced_img = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "    return Image.fromarray(enhanced_img)\n",
        "\n",
        "# ============================\n",
        "# 2. Dataset\n",
        "# ============================\n",
        "class PapilaDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None, use_clahe=False):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.use_clahe = use_clahe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"Image_Path\"]).convert(\"RGB\")\n",
        "        if self.use_clahe:\n",
        "            img = apply_clahe(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img   # DDPM only needs images\n",
        "\n",
        "# ============================\n",
        "# 3. Transforms\n",
        "# ============================\n",
        "transform = T.Compose([\n",
        "    T.Resize((128, 128)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5] * 3, [0.5] * 3)\n",
        "])\n",
        "\n",
        "# ============================\n",
        "# 4. Train/Val Split\n",
        "# ============================\n",
        "dataset = PapilaDataset(train_df, transform=transform, use_clahe=True)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "# ============================\n",
        "# 5. Model\n",
        "# ============================\n",
        "model = UNet2DModel(\n",
        "    sample_size=128,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(64, 128, 256),\n",
        "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\"),\n",
        "    up_block_types=(\"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
        ")\n",
        "\n",
        "# ============================\n",
        "# 6. Setup\n",
        "# ============================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "os.makedirs(\"samples\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# ============================\n",
        "# 7. Helpers\n",
        "# ============================\n",
        "def unnormalize(tensor):\n",
        "    return (tensor * 0.5 + 0.5).clamp(0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_samples(model, epoch, num_samples=9):\n",
        "    model.eval()\n",
        "    img = torch.randn((num_samples, 3, 128, 128), device=device)\n",
        "    for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
        "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "        with autocast():\n",
        "            noise_pred = model(img, t_batch).sample\n",
        "        img = noise_scheduler.step(noise_pred, t, img).prev_sample\n",
        "    img = unnormalize(img)\n",
        "    save_image(img, f\"samples/gen_epoch{epoch}.png\", nrow=3)\n",
        "    print(f\"Saved sample image at epoch {epoch}\")\n",
        "    model.train()\n",
        "    return f\"samples/gen_epoch{epoch}.png\"\n",
        "\n",
        "# ============================\n",
        "# 8. Training Loop\n",
        "# ============================\n",
        "epochs = 350\n",
        "sample_every = 10\n",
        "best_val_loss = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # ---- Training ----\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        clean_images = batch.to(device)\n",
        "        noise = torch.randn_like(clean_images)\n",
        "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
        "                                  (clean_images.size(0),), device=device).long()\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        with autocast():\n",
        "            noise_pred = model(noisy_images, timesteps).sample\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            imgs = batch.to(device)\n",
        "            noise = torch.randn_like(imgs)\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps,\n",
        "                                      (imgs.size(0),), device=device).long()\n",
        "            noisy = noise_scheduler.add_noise(imgs, noise, timesteps)\n",
        "            with autocast():\n",
        "                pred = model(noisy, timesteps).sample\n",
        "                val_loss += F.mse_loss(pred, noise).item() * imgs.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {loss.item():.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # ---- Save checkpoints ----\n",
        "    torch.save(model.state_dict(), f\"checkpoints/ddpm_epoch{epoch}.pt\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), \"checkpoints/ddpm_best.pt\")\n",
        "        print(f\"New best checkpoint at epoch {epoch} | Val Loss {best_val_loss:.4f}\")\n",
        "\n",
        "    # ---- Save samples ----\n",
        "    if epoch % sample_every == 0:\n",
        "        generate_samples(model, epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49cbe254",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/syma/custom_tmp/ipykernel_415090/939310682.py:123: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/home/syma/custom_tmp/ipykernel_415090/939310682.py:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/home/syma/custom_tmp/ipykernel_415090/939310682.py:196: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001/300 | train 0.1535 | val 0.1489\n",
            " New best checkpoint (val=0.1489)\n",
            "Epoch 002/300 | train 0.0463 | val 0.0740\n",
            " New best checkpoint (val=0.0740)\n",
            "Epoch 003/300 | train 0.0481 | val 0.0499\n",
            " New best checkpoint (val=0.0499)\n",
            "Epoch 004/300 | train 0.0268 | val 0.0603\n",
            "Epoch 005/300 | train 0.0380 | val 0.0328\n",
            " New best checkpoint (val=0.0328)\n",
            "Epoch 006/300 | train 0.0255 | val 0.0316\n",
            " New best checkpoint (val=0.0316)\n",
            "Epoch 007/300 | train 0.0171 | val 0.0507\n",
            "Epoch 008/300 | train 0.0242 | val 0.0297\n",
            " New best checkpoint (val=0.0297)\n",
            "Epoch 009/300 | train 0.0387 | val 0.0341\n",
            "Epoch 010/300 | train 0.0138 | val 0.0346\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/syma/custom_tmp/ipykernel_415090/939310682.py:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved samples: samples1/gen_epoch10.png\n",
            "Epoch 011/300 | train 0.0612 | val 0.0390\n",
            "Epoch 012/300 | train 0.0538 | val 0.0269\n",
            " New best checkpoint (val=0.0269)\n",
            "Epoch 013/300 | train 0.0193 | val 0.0387\n",
            "Epoch 014/300 | train 0.0495 | val 0.0335\n",
            "Epoch 015/300 | train 0.0129 | val 0.0305\n",
            "Epoch 016/300 | train 0.0140 | val 0.0317\n",
            "Epoch 017/300 | train 0.0172 | val 0.0304\n",
            "Epoch 018/300 | train 0.0176 | val 0.0270\n",
            "Epoch 019/300 | train 0.0127 | val 0.0241\n",
            " New best checkpoint (val=0.0241)\n",
            "Epoch 020/300 | train 0.0227 | val 0.0190\n",
            " New best checkpoint (val=0.0190)\n",
            "Saved samples: samples1/gen_epoch20.png\n",
            "Epoch 021/300 | train 0.1067 | val 0.0184\n",
            " New best checkpoint (val=0.0184)\n",
            "Epoch 022/300 | train 0.0128 | val 0.0125\n",
            " New best checkpoint (val=0.0125)\n",
            "Epoch 023/300 | train 0.0161 | val 0.0263\n",
            "Epoch 024/300 | train 0.0201 | val 0.0234\n",
            "Epoch 025/300 | train 0.0164 | val 0.0189\n",
            "Epoch 026/300 | train 0.0231 | val 0.0298\n",
            "Epoch 027/300 | train 0.0186 | val 0.0191\n",
            "Epoch 028/300 | train 0.0412 | val 0.0141\n",
            "Epoch 029/300 | train 0.0074 | val 0.0211\n",
            "Epoch 030/300 | train 0.0090 | val 0.0169\n",
            "Saved samples: samples1/gen_epoch30.png\n",
            "Epoch 031/300 | train 0.0141 | val 0.0344\n",
            "Epoch 032/300 | train 0.0103 | val 0.0319\n",
            "Epoch 033/300 | train 0.0062 | val 0.0216\n",
            "Epoch 034/300 | train 0.0240 | val 0.0185\n",
            "Epoch 035/300 | train 0.0048 | val 0.0250\n",
            "Epoch 036/300 | train 0.0156 | val 0.0154\n",
            "Epoch 037/300 | train 0.0084 | val 0.0261\n",
            "Epoch 038/300 | train 0.0149 | val 0.0130\n",
            "Epoch 039/300 | train 0.0329 | val 0.0216\n",
            "Epoch 040/300 | train 0.0152 | val 0.0165\n",
            "Saved samples: samples1/gen_epoch40.png\n",
            "Epoch 041/300 | train 0.0106 | val 0.0276\n",
            "Epoch 042/300 | train 0.0085 | val 0.0209\n",
            "Epoch 043/300 | train 0.0219 | val 0.0216\n",
            "Epoch 044/300 | train 0.0109 | val 0.0168\n",
            "Epoch 045/300 | train 0.0204 | val 0.0141\n",
            "Epoch 046/300 | train 0.0058 | val 0.0232\n",
            "Epoch 047/300 | train 0.0123 | val 0.0296\n",
            "Epoch 048/300 | train 0.0069 | val 0.0244\n",
            "Epoch 049/300 | train 0.0126 | val 0.0272\n",
            "Epoch 050/300 | train 0.0192 | val 0.0269\n",
            "Saved samples: samples1/gen_epoch50.png\n",
            "Epoch 051/300 | train 0.0138 | val 0.0312\n",
            "Epoch 052/300 | train 0.0055 | val 0.0192\n",
            "Epoch 053/300 | train 0.0098 | val 0.0260\n",
            "Epoch 054/300 | train 0.0102 | val 0.0120\n",
            " New best checkpoint (val=0.0120)\n",
            "Epoch 055/300 | train 0.0146 | val 0.0166\n",
            "Epoch 056/300 | train 0.0088 | val 0.0273\n",
            "Epoch 057/300 | train 0.0052 | val 0.0168\n",
            "Epoch 058/300 | train 0.0078 | val 0.0237\n",
            "Epoch 059/300 | train 0.0220 | val 0.0204\n",
            "Epoch 060/300 | train 0.0602 | val 0.0175\n",
            "Saved samples: samples1/gen_epoch60.png\n",
            "Epoch 061/300 | train 0.0256 | val 0.0172\n",
            "Epoch 062/300 | train 0.0081 | val 0.0182\n",
            "Epoch 063/300 | train 0.0101 | val 0.0193\n",
            "Epoch 064/300 | train 0.0053 | val 0.0247\n",
            "Epoch 065/300 | train 0.0184 | val 0.0194\n",
            "Epoch 066/300 | train 0.0253 | val 0.0257\n",
            "Epoch 067/300 | train 0.0061 | val 0.0111\n",
            " New best checkpoint (val=0.0111)\n",
            "Epoch 068/300 | train 0.0158 | val 0.0140\n",
            "Epoch 069/300 | train 0.0080 | val 0.0177\n",
            "Epoch 070/300 | train 0.0230 | val 0.0153\n",
            "Saved samples: samples1/gen_epoch70.png\n",
            "Epoch 071/300 | train 0.0053 | val 0.0094\n",
            " New best checkpoint (val=0.0094)\n",
            "Epoch 072/300 | train 0.0093 | val 0.0171\n",
            "Epoch 073/300 | train 0.0066 | val 0.0169\n",
            "Epoch 074/300 | train 0.0092 | val 0.0180\n",
            "Epoch 075/300 | train 0.0355 | val 0.0149\n",
            "Epoch 076/300 | train 0.0042 | val 0.0122\n",
            "Epoch 077/300 | train 0.0057 | val 0.0301\n",
            "Epoch 078/300 | train 0.0114 | val 0.0125\n",
            "Epoch 079/300 | train 0.0098 | val 0.0215\n",
            "Epoch 080/300 | train 0.0078 | val 0.0214\n",
            "Saved samples: samples1/gen_epoch80.png\n",
            "Epoch 081/300 | train 0.0171 | val 0.0193\n",
            "Epoch 082/300 | train 0.0168 | val 0.0225\n",
            "Epoch 083/300 | train 0.0042 | val 0.0205\n",
            "Epoch 084/300 | train 0.0124 | val 0.0222\n",
            "Epoch 085/300 | train 0.0070 | val 0.0176\n",
            "Epoch 086/300 | train 0.0107 | val 0.0100\n",
            "Epoch 087/300 | train 0.0404 | val 0.0328\n",
            "Epoch 088/300 | train 0.0197 | val 0.0164\n",
            "Epoch 089/300 | train 0.0226 | val 0.0170\n",
            "Epoch 090/300 | train 0.0173 | val 0.0108\n",
            "Saved samples: samples1/gen_epoch90.png\n",
            "Epoch 091/300 | train 0.0184 | val 0.0168\n",
            "Epoch 092/300 | train 0.0409 | val 0.0191\n",
            "Epoch 093/300 | train 0.0643 | val 0.0200\n",
            "Epoch 094/300 | train 0.0049 | val 0.0090\n",
            " New best checkpoint (val=0.0090)\n",
            "Epoch 095/300 | train 0.0052 | val 0.0302\n",
            "Epoch 096/300 | train 0.0201 | val 0.0100\n",
            "Epoch 097/300 | train 0.0067 | val 0.0235\n",
            "Epoch 098/300 | train 0.0078 | val 0.0195\n",
            "Epoch 099/300 | train 0.0226 | val 0.0114\n",
            "Epoch 100/300 | train 0.0111 | val 0.0253\n",
            "Saved samples: samples1/gen_epoch100.png\n",
            "Epoch 101/300 | train 0.0082 | val 0.0269\n",
            "Epoch 102/300 | train 0.0072 | val 0.0228\n",
            "Epoch 103/300 | train 0.0072 | val 0.0088\n",
            " New best checkpoint (val=0.0088)\n",
            "Epoch 104/300 | train 0.0089 | val 0.0157\n",
            "Epoch 105/300 | train 0.0269 | val 0.0191\n",
            "Epoch 106/300 | train 0.0070 | val 0.0112\n",
            "Epoch 107/300 | train 0.0041 | val 0.0098\n",
            "Epoch 108/300 | train 0.0057 | val 0.0222\n",
            "Epoch 109/300 | train 0.0080 | val 0.0114\n",
            "Epoch 110/300 | train 0.0084 | val 0.0197\n",
            "Saved samples: samples1/gen_epoch110.png\n",
            "Epoch 111/300 | train 0.0183 | val 0.0243\n",
            "Epoch 112/300 | train 0.0055 | val 0.0160\n",
            "Epoch 113/300 | train 0.0036 | val 0.0203\n",
            "Epoch 114/300 | train 0.0106 | val 0.0185\n",
            "Epoch 115/300 | train 0.0143 | val 0.0299\n",
            "Epoch 116/300 | train 0.0148 | val 0.0167\n",
            "Epoch 117/300 | train 0.0071 | val 0.0156\n",
            "Epoch 118/300 | train 0.0136 | val 0.0140\n",
            "Epoch 119/300 | train 0.0160 | val 0.0091\n",
            "Epoch 120/300 | train 0.0170 | val 0.0125\n",
            "Saved samples: samples1/gen_epoch120.png\n",
            "Epoch 121/300 | train 0.0115 | val 0.0167\n",
            "Epoch 122/300 | train 0.0082 | val 0.0244\n",
            "Epoch 123/300 | train 0.0050 | val 0.0185\n",
            "Epoch 124/300 | train 0.0232 | val 0.0108\n",
            "Epoch 125/300 | train 0.0348 | val 0.0172\n",
            "Epoch 126/300 | train 0.0152 | val 0.0170\n",
            "Epoch 127/300 | train 0.0050 | val 0.0112\n",
            "Epoch 128/300 | train 0.0139 | val 0.0141\n",
            "Epoch 129/300 | train 0.0062 | val 0.0193\n",
            "Epoch 130/300 | train 0.0053 | val 0.0139\n",
            "Saved samples: samples1/gen_epoch130.png\n",
            "Epoch 131/300 | train 0.0037 | val 0.0117\n",
            "Epoch 132/300 | train 0.0216 | val 0.0167\n",
            "Epoch 133/300 | train 0.0045 | val 0.0232\n",
            "Epoch 134/300 | train 0.0045 | val 0.0133\n",
            "Epoch 135/300 | train 0.0109 | val 0.0187\n",
            "Epoch 136/300 | train 0.0051 | val 0.0144\n",
            "Epoch 137/300 | train 0.0497 | val 0.0166\n",
            "Epoch 138/300 | train 0.0111 | val 0.0075\n",
            " New best checkpoint (val=0.0075)\n",
            "Epoch 139/300 | train 0.0444 | val 0.0128\n",
            "Epoch 140/300 | train 0.0070 | val 0.0165\n",
            "Saved samples: samples1/gen_epoch140.png\n",
            "Epoch 141/300 | train 0.0366 | val 0.0175\n",
            "Epoch 142/300 | train 0.0282 | val 0.0139\n",
            "Epoch 143/300 | train 0.0070 | val 0.0123\n",
            "Epoch 144/300 | train 0.0048 | val 0.0123\n",
            "Epoch 145/300 | train 0.0140 | val 0.0156\n",
            "Epoch 146/300 | train 0.0123 | val 0.0164\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at inline_container.cc:603] . unexpected pos 9681152 vs 9681040",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[0;32m~/Python-3.10.0/myenv/lib/python3.10/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/Python-3.10.0/myenv/lib/python3.10/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/176: file write failed",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[82], line 204\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# ---- Save Checkpoints ----\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints1/ddpm_epoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val:\n\u001b[1;32m    207\u001b[0m     best_val \u001b[38;5;241m=\u001b[39m val_loss\n",
            "File \u001b[0;32m~/Python-3.10.0/myenv/lib/python3.10/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/Python-3.10.0/myenv/lib/python3.10/site-packages/torch/serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 9681152 vs 9681040"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.utils import save_image\n",
        "from diffusers import UNet2DModel, DDPMScheduler\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from skimage.exposure import match_histograms\n",
        "\n",
        "# =====================================================\n",
        "# 1. Reproducibility\n",
        "# =====================================================\n",
        "def set_seed(s=42):\n",
        "    torch.manual_seed(s)\n",
        "    np.random.seed(s)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "set_seed(42)\n",
        "\n",
        "# =====================================================\n",
        "# 2. Preprocessing\n",
        "# =====================================================\n",
        "def apply_clahe(pil_img, clip=1.6, grid=8):\n",
        "    img = np.array(pil_img)\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(grid, grid))\n",
        "    cl = clahe.apply(l)\n",
        "    merged = cv2.merge((cl, a, b))\n",
        "    out = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)\n",
        "    return Image.fromarray(out)\n",
        "\n",
        "def hist_match_img(pil_img, ref_pil):\n",
        "    \"\"\"Match colors to a reference image\"\"\"\n",
        "    src = np.asarray(pil_img)\n",
        "    ref = np.asarray(ref_pil)\n",
        "    matched = match_histograms(src, ref, channel_axis=-1)\n",
        "    return Image.fromarray(matched.astype(np.uint8))\n",
        "\n",
        "# =====================================================\n",
        "# 3. Dataset\n",
        "# =====================================================\n",
        "class PapilaDataset(Dataset):\n",
        "    def __init__(self, df, transform=None,\n",
        "                 use_clahe=True, use_hist=False, ref_img=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.t = transform\n",
        "        self.use_clahe = use_clahe\n",
        "        self.use_hist = use_hist\n",
        "        self.ref = ref_img\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        p = self.df.iloc[i][\"Image_Path\"]\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "\n",
        "        if self.use_clahe:\n",
        "            img = apply_clahe(img)\n",
        "        if self.use_hist and self.ref is not None:\n",
        "            img = hist_match_img(img, self.ref)\n",
        "        if self.t:\n",
        "            img = self.t(img)\n",
        "\n",
        "        return img   # unconditional diffusion needs only images\n",
        "\n",
        "# =====================================================\n",
        "# 4. Transforms\n",
        "# =====================================================\n",
        "transform = T.Compose([\n",
        "    T.Resize(160),\n",
        "    T.CenterCrop(128),  \n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.5]*3, [0.5]*3)  # [-1, 1]\n",
        "])\n",
        "\n",
        "# (choose a good reference fundus for color normalization)\n",
        "REF_PATH = \"papila_dataset/PapilaDB-PAPILA-17f8fa7746adb20275b5b6a0d99dc9dfe3007e9f/FundusImages/RET033OS.jpg\"  \n",
        "ref_img = Image.open(REF_PATH).convert(\"RGB\") if os.path.exists(REF_PATH) else None\n",
        "\n",
        "# =====================================================\n",
        "# 5. Dataset Split + DataLoader\n",
        "# =====================================================\n",
        "full_ds = PapilaDataset(train_df, transform=transform,\n",
        "                        use_clahe=True, use_hist=(ref_img is not None), ref_img=ref_img)\n",
        "\n",
        "n_train = int(0.8 * len(full_ds))\n",
        "n_val = len(full_ds) - n_train\n",
        "train_ds, val_ds = random_split(full_ds, [n_train, n_val])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "# =====================================================\n",
        "# 6. Model\n",
        "# =====================================================\n",
        "model = UNet2DModel(\n",
        "    sample_size=128,\n",
        "    in_channels=3,\n",
        "    out_channels=3,\n",
        "    layers_per_block=2,\n",
        "    block_out_channels=(64, 128, 128),\n",
        "    down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
        "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
        ")\n",
        "model.gradient_checkpointing = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000,\n",
        "    beta_schedule=\"squaredcos_cap_v2\",\n",
        "    prediction_type=\"epsilon\"\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "os.makedirs(\"samples1\", exist_ok=True)\n",
        "os.makedirs(\"checkpoints1\", exist_ok=True)\n",
        "\n",
        "# =====================================================\n",
        "# 7. Helpers\n",
        "# =====================================================\n",
        "def unnormalize(x): \n",
        "    return (x*0.5 + 0.5).clamp(0,1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_samples(model, epoch, num=9, color_ref=None):\n",
        "    \"\"\"Generate synthetic images and color-correct them to reference.\"\"\"\n",
        "    model.eval()\n",
        "    x = torch.randn((num, 3, 128, 128), device=device)\n",
        "    for t in reversed(range(noise_scheduler.num_train_timesteps)):\n",
        "        tt = torch.full((num,), t, device=device, dtype=torch.long)\n",
        "        with autocast():\n",
        "            eps = model(x, tt).sample\n",
        "        x = noise_scheduler.step(eps, t, x).prev_sample\n",
        "    \n",
        "    # unnormalize to [0,1] and convert to PIL\n",
        "    imgs = unnormalize(x).cpu()\n",
        "    pil_imgs = [T.ToPILImage()(im) for im in imgs]\n",
        "\n",
        "    # optional color normalization\n",
        "    if color_ref is not None:\n",
        "        pil_imgs = [hist_match_img(im, color_ref) for im in pil_imgs]\n",
        "\n",
        "    # save grid\n",
        "    grid = torch.stack([T.ToTensor()(im) for im in pil_imgs])\n",
        "    save_image(grid, f\"samples1/gen_epoch{epoch}.png\", nrow=3)\n",
        "    print(f\"Saved samples: samples1/gen_epoch{epoch}.png\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "# =====================================================\n",
        "# 8. Training Loop\n",
        "# =====================================================\n",
        "epochs = 300\n",
        "sample_every = 10\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    # ---- Training ----\n",
        "    model.train()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (linuxlab)",
      "language": "python",
      "name": "linuxlab"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
